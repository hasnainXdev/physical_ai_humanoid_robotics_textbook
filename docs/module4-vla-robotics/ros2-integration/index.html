<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module4-vla-robotics/ros2-integration" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">ROS 2 Integration for VLA Pipelines | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://physical-ai-humanoid-robotics-textb-tau.vercel.app/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://physical-ai-humanoid-robotics-textb-tau.vercel.app/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://physical-ai-humanoid-robotics-textb-tau.vercel.app/docs/module4-vla-robotics/ros2-integration"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="ROS 2 Integration for VLA Pipelines | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="The Role of ROS 2 in VLA Robotics"><meta data-rh="true" property="og:description" content="The Role of ROS 2 in VLA Robotics"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://physical-ai-humanoid-robotics-textb-tau.vercel.app/docs/module4-vla-robotics/ros2-integration"><link data-rh="true" rel="alternate" href="https://physical-ai-humanoid-robotics-textb-tau.vercel.app/docs/module4-vla-robotics/ros2-integration" hreflang="en"><link data-rh="true" rel="alternate" href="https://physical-ai-humanoid-robotics-textb-tau.vercel.app/docs/module4-vla-robotics/ros2-integration" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"ROS 2 Integration for VLA Pipelines","item":"https://physical-ai-humanoid-robotics-textb-tau.vercel.app/docs/module4-vla-robotics/ros2-integration"}]}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Physical AI &amp; Humanoid Robotics RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Physical AI &amp; Humanoid Robotics Atom Feed"><link rel="stylesheet" href="/assets/css/styles.87aa9449.css">
<script src="/assets/js/runtime~main.01f656c7.js" defer="defer"></script>
<script src="/assets/js/main.d97914f4.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/module1-ros2/introduction">Tutorial</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/module1-ros2/introduction"><span title="ROS 2 Nervous System" class="categoryLinkLabel_W154">ROS 2 Nervous System</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/module2-digital-twins/01-introduction"><span title="Digital Twins (Gazebo + Unity)" class="categoryLinkLabel_W154">Digital Twins (Gazebo + Unity)</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/module3-nvidia-isaac-sim/01-introduction"><span title="NVIDIA Isaac Sim + VSLAM + Nav2" class="categoryLinkLabel_W154">NVIDIA Isaac Sim + VSLAM + Nav2</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/module4-vla-robotics/introduction"><span title="Vision-Language-Action (VLA) Robotics" class="categoryLinkLabel_W154">Vision-Language-Action (VLA) Robotics</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/module4-vla-robotics/introduction"><span title="Introduction to LLM-Driven Robotics" class="linkLabel_WmDU">Introduction to LLM-Driven Robotics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/module4-vla-robotics/whisper"><span title="Voice Commands with Whisper" class="linkLabel_WmDU">Voice Commands with Whisper</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/module4-vla-robotics/action-graphs"><span title="Natural Language to Robotic Action Graphs" class="linkLabel_WmDU">Natural Language to Robotic Action Graphs</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/module4-vla-robotics/cognitive-planners"><span title="Cognitive Planners for Complex Tasks" class="linkLabel_WmDU">Cognitive Planners for Complex Tasks</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/module4-vla-robotics/breaking-down-language"><span title="Breaking Down Language: From Utterance to Robot Action" class="linkLabel_WmDU">Breaking Down Language: From Utterance to Robot Action</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/module4-vla-robotics/ros2-integration"><span title="ROS 2 Integration for VLA Pipelines" class="linkLabel_WmDU">ROS 2 Integration for VLA Pipelines</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/docs/module4-vla-robotics/diagrams/cognitive-planner-flow"><span title="diagrams" class="categoryLinkLabel_W154">diagrams</span></a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/capstone-humanoid-robot/overview"><span title="Capstone - Autonomous Humanoid Robot" class="categoryLinkLabel_W154">Capstone - Autonomous Humanoid Robot</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro"><span title="**Physical AI &amp; Humanoid Robotics**" class="linkLabel_WmDU">**Physical AI &amp; Humanoid Robotics**</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language-Action (VLA) Robotics</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">ROS 2 Integration for VLA Pipelines</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div style="display:flex;justify-content:space-between"><div class="personalizationContainer_KDQQ"><button>Personalize Chapter for Me</button></div><div class="translatorContainer_PjNM"><button>Urdu Translation</button></div></div><div class="theme-doc-markdown markdown"><header><h1>ROS 2 Integration for VLA Pipelines</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-role-of-ros-2-in-vla-robotics">The Role of ROS 2 in VLA Robotics<a href="#the-role-of-ros-2-in-vla-robotics" class="hash-link" aria-label="Direct link to The Role of ROS 2 in VLA Robotics" title="Direct link to The Role of ROS 2 in VLA Robotics" translate="no">â€‹</a></h2>
<p>Robot Operating System 2 (ROS 2) serves as a foundational middleware for building complex robotic systems. Its distributed, modular, and real-time capabilities make it an ideal platform for integrating the various components of a Vision-Language-Action (VLA) pipeline. From managing sensor data streams and control commands to facilitating communication between LLM-driven cognitive planners and low-level robot actuators, ROS 2 provides the essential infrastructure for orchestrating intelligent humanoid robot behaviors. This chapter focuses on how to effectively integrate natural language processing and LLM-driven decision-making within the ROS 2 ecosystem.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="why-ros-2-for-vla">Why ROS 2 for VLA?<a href="#why-ros-2-for-vla" class="hash-link" aria-label="Direct link to Why ROS 2 for VLA?" title="Direct link to Why ROS 2 for VLA?" translate="no">â€‹</a></h3>
<ul>
<li class=""><strong>Modularity:</strong> ROS 2&#x27;s node-based architecture allows individual VLA components (e.g., Whisper transcription, LLM planner, action executor) to run as separate processes, promoting code reusability and maintainability.</li>
<li class=""><strong>Interprocess Communication:</strong> DDS (Data Distribution Service) provides robust and efficient communication mechanisms (topics, services, actions) for data exchange between VLA modules.</li>
<li class=""><strong>Real-time Performance:</strong> Designed with real-time requirements in mind, crucial for responsive human-robot interaction and dynamic task execution.</li>
<li class=""><strong>Extensibility:</strong> A vast ecosystem of existing ROS 2 packages for perception, navigation, manipulation, and simulation can be readily integrated into VLA pipelines.</li>
<li class=""><strong>Hardware Abstraction:</strong> ROS 2 provides a standardized way to interact with diverse robot hardware, simplifying the transition from simulation to real-world deployment.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="designing-a-ros-2-vla-pipeline">Designing a ROS 2 VLA Pipeline<a href="#designing-a-ros-2-vla-pipeline" class="hash-link" aria-label="Direct link to Designing a ROS 2 VLA Pipeline" title="Direct link to Designing a ROS 2 VLA Pipeline" translate="no">â€‹</a></h2>
<p>A typical ROS 2 VLA pipeline involves several interconnected nodes, each responsible for a specific function:</p>
<ol>
<li class="">
<p><strong>Audio Input &amp; Speech-to-Text Node:</strong></p>
<ul>
<li class=""><strong>Function:</strong> Captures audio from microphones and uses a speech-to-text model (e.g., Whisper) to transcribe spoken commands into text.</li>
<li class=""><strong>Inputs:</strong> Raw audio data (e.g., <code>audio_common_msgs/AudioData</code>).</li>
<li class=""><strong>Outputs:</strong> Transcribed text (e.g., <code>std_msgs/String</code>).</li>
</ul>
</li>
<li class="">
<p><strong>LLM Interface / Cognitive Planner Node:</strong></p>
<ul>
<li class=""><strong>Function:</strong> Receives transcribed text, integrates with a Large Language Model for intent recognition, task decomposition, and high-level planning. It translates natural language into a sequence of abstract robotic actions or a symbolic plan.</li>
<li class=""><strong>Inputs:</strong> Transcribed text (<code>std_msgs/String</code>), perception data (e.g., <code>sensor_msgs/Image</code>, <code>sensor_msgs/PointCloud2</code>), current robot state (<code>nav_msgs/Odometry</code>, <code>sensor_msgs/JointState</code>).</li>
<li class=""><strong>Outputs:</strong> High-level action commands (e.g., custom ROS 2 messages defining navigation goals, manipulation targets, or abstract behaviors).</li>
</ul>
</li>
<li class="">
<p><strong>Action Executor Node(s):</strong></p>
<ul>
<li class=""><strong>Function:</strong> Receives high-level action commands from the cognitive planner and translates them into low-level robot control signals. This often involves interacting with existing ROS 2 navigation (Nav2) and manipulation stacks.</li>
<li class=""><strong>Inputs:</strong> High-level action commands (custom messages).</li>
<li class=""><strong>Outputs:</strong> Low-level control commands (e.g., <code>geometry_msgs/Twist</code> for velocity, <code>sensor_msgs/JointState</code> for joint positions, <code>control_msgs/GripperCommand</code> for grippers).</li>
</ul>
</li>
<li class="">
<p><strong>Perception Nodes:</strong></p>
<ul>
<li class=""><strong>Function:</strong> Process raw sensor data (cameras, LiDAR) into meaningful information for the LLM and action executor (e.g., object detections, segmented images, depth maps, scene graphs).</li>
<li class=""><strong>Outputs:</strong> Processed sensor data (e.g., <code>vision_msgs/Detection2DArray</code>, <code>sensor_msgs/PointCloud2</code>).</li>
</ul>
</li>
<li class="">
<p><strong>Robot State &amp; Feedback Node:</strong></p>
<ul>
<li class=""><strong>Function:</strong> Publishes the robot&#x27;s current state (pose, joint states) and provides feedback on action execution status to the cognitive planner and human user.</li>
<li class=""><strong>Inputs:</strong> Raw sensor data, robot internal state.</li>
<li class=""><strong>Outputs:</strong> Robot state (<code>nav_msgs/Odometry</code>, <code>sensor_msgs/JointState</code>), feedback messages (<code>std_msgs/String</code>).</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="communication-patterns-in-ros-2-vla">Communication Patterns in ROS 2 VLA<a href="#communication-patterns-in-ros-2-vla" class="hash-link" aria-label="Direct link to Communication Patterns in ROS 2 VLA" title="Direct link to Communication Patterns in ROS 2 VLA" translate="no">â€‹</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="topics">Topics:<a href="#topics" class="hash-link" aria-label="Direct link to Topics:" title="Direct link to Topics:" translate="no">â€‹</a></h3>
<ul>
<li class=""><strong>Usage:</strong> Unidirectional streaming of data (e.g., audio, transcribed text, sensor data, robot state).</li>
<li class=""><strong>Example:</strong> Audio capture node publishes to <code>/audio/raw</code>, Whisper node subscribes to <code>/audio/raw</code> and publishes to <code>/voice_command/text</code>.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="services">Services:<a href="#services" class="hash-link" aria-label="Direct link to Services:" title="Direct link to Services:" translate="no">â€‹</a></h3>
<ul>
<li class=""><strong>Usage:</strong> Request-response communication for specific, often idempotent, operations (e.g., querying object properties, triggering a specific low-level behavior).</li>
<li class=""><strong>Example:</strong> LLM planner might call a service <code>/get_object_info</code> to get details about a perceived object.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="actions">Actions:<a href="#actions" class="hash-link" aria-label="Direct link to Actions:" title="Direct link to Actions:" translate="no">â€‹</a></h3>
<ul>
<li class=""><strong>Usage:</strong> Long-running, goal-oriented tasks with feedback and preemption capabilities (e.g., <code>NavigateToPose</code>, <code>PickAndPlace</code>). Ideal for higher-level robot behaviors.</li>
<li class=""><strong>Example:</strong> The cognitive planner might send a <code>NavigateToPose</code> action goal to Nav2, receiving continuous feedback on progress.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="practical-example-robot-fetch-the-blue-cube-from-the-table">Practical Example: &quot;Robot, fetch the blue cube from the table.&quot;<a href="#practical-example-robot-fetch-the-blue-cube-from-the-table" class="hash-link" aria-label="Direct link to Practical Example: &quot;Robot, fetch the blue cube from the table.&quot;" title="Direct link to Practical Example: &quot;Robot, fetch the blue cube from the table.&quot;" translate="no">â€‹</a></h2>
<ol>
<li class=""><strong>Human:</strong> &quot;Robot, fetch the blue cube from the table.&quot;</li>
<li class=""><strong>Audio Node:</strong> Captures audio, publishes to <code>/audio/raw</code>.</li>
<li class=""><strong>Whisper Node:</strong> Subscribes to <code>/audio/raw</code>, transcribes to &quot;fetch the blue cube from the table,&quot; publishes to <code>/voice_command/text</code>.</li>
<li class=""><strong>LLM Planner Node:</strong>
<ul>
<li class="">Subscribes to <code>/voice_command/text</code>.</li>
<li class="">Uses LLM to interpret intent: <code>FETCH</code>.</li>
<li class="">Identifies <code>object: blue_cube</code>, <code>location: table</code>.</li>
<li class="">Queries Perception Node via service for <code>blue_cube</code>&#x27;s precise pose.</li>
<li class="">Decomposes <code>FETCH</code> into: <code>NAVIGATE(table)</code>, <code>PICK(blue_cube)</code>, <code>NAVIGATE(human_location)</code>, <code>PLACE(blue_cube)</code>.</li>
<li class="">Sends <code>NAVIGATE(table_pose)</code> action goal to Nav2.</li>
</ul>
</li>
<li class=""><strong>Nav2 Action Server:</strong> Executes navigation to the table.</li>
<li class=""><strong>LLM Planner Node:</strong> Monitors Nav2 feedback; once <code>NAVIGATE</code> is complete, sends <code>PICK(blue_cube_pose)</code> action goal to Manipulation Node.</li>
<li class=""><strong>Manipulation Node:</strong> Executes grasp, using perceived <code>blue_cube_pose</code>.</li>
<li class=""><strong>LLM Planner Node:</strong> Monitors Manipulation feedback; once <code>PICK</code> is complete, sends <code>NAVIGATE(human_pose)</code> action goal to Nav2.</li>
<li class=""><strong>... and so on.</strong></li>
</ol>
<!-- -->
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion" translate="no">â€‹</a></h2>
<p>ROS 2 provides an indispensable framework for integrating the diverse components of VLA pipelines, enabling humanoid robots to process natural language, reason about tasks, and execute complex actions in the physical world. By leveraging ROS 2&#x27;s modularity, communication mechanisms, and extensive ecosystem, developers can build robust, scalable, and intelligent robotic systems that bridge the gap between human intent and robot capabilities.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4-vla-robotics/06-ros2-integration.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/module4-vla-robotics/breaking-down-language"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Breaking Down Language: From Utterance to Robot Action</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/module4-vla-robotics/diagrams/cognitive-planner-flow"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">cognitive-planner-flow</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-role-of-ros-2-in-vla-robotics" class="table-of-contents__link toc-highlight">The Role of ROS 2 in VLA Robotics</a><ul><li><a href="#why-ros-2-for-vla" class="table-of-contents__link toc-highlight">Why ROS 2 for VLA?</a></li></ul></li><li><a href="#designing-a-ros-2-vla-pipeline" class="table-of-contents__link toc-highlight">Designing a ROS 2 VLA Pipeline</a></li><li><a href="#communication-patterns-in-ros-2-vla" class="table-of-contents__link toc-highlight">Communication Patterns in ROS 2 VLA</a><ul><li><a href="#topics" class="table-of-contents__link toc-highlight">Topics:</a></li><li><a href="#services" class="table-of-contents__link toc-highlight">Services:</a></li><li><a href="#actions" class="table-of-contents__link toc-highlight">Actions:</a></li></ul></li><li><a href="#practical-example-robot-fetch-the-blue-cube-from-the-table" class="table-of-contents__link toc-highlight">Practical Example: &quot;Robot, fetch the blue cube from the table.&quot;</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://x.com/hasnainxdev" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/hasnainxdev" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2025 Physical AI & Humanoid Robotics. Built with ðŸ’– Muhammad Hasnain</div></div></div></footer></div>
</body>
</html>