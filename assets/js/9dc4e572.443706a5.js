"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[9832],{6845:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"capstone-humanoid-robot/full-task-execution","title":"Full Task Execution in Simulation","description":"Objective","source":"@site/docs/capstone-humanoid-robot/06-full-task-execution.mdx","sourceDirName":"capstone-humanoid-robot","slug":"/capstone-humanoid-robot/full-task-execution","permalink":"/physical_ai_humanoid_robotics_textbook/docs/capstone-humanoid-robot/full-task-execution","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/capstone-humanoid-robot/06-full-task-execution.mdx","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Object Manipulation with ROS Control","permalink":"/physical_ai_humanoid_robotics_textbook/docs/capstone-humanoid-robot/object-manipulation"},"next":{"title":"**Physical AI & Humanoid Robotics**","permalink":"/physical_ai_humanoid_robotics_textbook/docs/intro"}}');var o=a(4848),i=a(8453);const s={sidebar_position:6},r="Full Task Execution in Simulation",l={},c=[{value:"Objective",id:"objective",level:2},{value:"Final System Architecture",id:"final-system-architecture",level:2},{value:"The Main Execution Logic",id:"the-main-execution-logic",level:2},{value:"<code>task_executor.py</code>",id:"task_executorpy",level:3},{value:"Integrated Code Blocks",id:"integrated-code-blocks",level:3},{value:"Cognitive Planner Node (<code>cognitive_planner_node.py</code>)",id:"cognitive-planner-node-cognitive_planner_nodepy",level:4},{value:"Manipulation Action Server (<code>manipulation_server.py</code>)",id:"manipulation-action-server-manipulation_serverpy",level:4},{value:"Running the Full Simulation",id:"running-the-full-simulation",level:2},{value:"Launch File: <code>capstone.launch.py</code>",id:"launch-file-capstonelaunchpy",level:3},{value:"Steps to Run",id:"steps-to-run",level:3},{value:"Example Task Walkthrough",id:"example-task-walkthrough",level:2},{value:"Capstone Task Flow",id:"capstone-task-flow",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"full-task-execution-in-simulation",children:"Full Task Execution in Simulation"})}),"\n",(0,o.jsx)(n.h2,{id:"objective",children:"Objective"}),"\n",(0,o.jsx)(n.p,{children:"This final section of the capstone project integrates all the individual components developed in the previous sections into a single, cohesive system. The goal is to achieve a full end-to-end execution of a complex task given by a natural language voice command. This involves orchestrating voice recognition, language understanding, task planning, navigation, perception, and manipulation."}),"\n",(0,o.jsx)(n.h2,{id:"final-system-architecture",children:"Final System Architecture"}),"\n",(0,o.jsx)(n.p,{children:"The final architecture is a ROS 2-based system where each component runs as a node, communicating through topics, services, and actions."}),"\n",(0,o.jsx)(n.mermaid,{value:"graph TD\n    A[User Voice Command] --\x3e B{Whisper Node};\n    B --\x3e C{LLM Cognitive Planner Node};\n    C -- Task Plan --\x3e D[Task Executor Node];\n    D -- Navigation Goal --\x3e E[Nav2];\n    D -- Detection Goal --\x3e F[Object Detection Node];\n    D -- Manipulation Goal --\x3e G[Manipulation Node];\n    E -- Robot Pose --\x3e H[SLAM];\n    F -- Object Pose --\x3e D;\n    G -- Manipulation Status --\x3e D;\n    I[Isaac Sim] -- Sensor Data --\x3e E;\n    I -- Camera Feed --\x3e F;\n    I -- Robot State --\x3e G;\n    subgraph ROS 2 System\n        B; C; D; E; F; G; H;\n    end\n    subgraph Simulation\n        I;\n    end\n    subgraph Human Interface\n        A;\n    end"}),"\n",(0,o.jsx)(n.h2,{id:"the-main-execution-logic",children:"The Main Execution Logic"}),"\n",(0,o.jsxs)(n.p,{children:["The core of the full task execution is a central executive script, ",(0,o.jsx)(n.code,{children:"task_executor.py"}),", which manages the state of the robot and sequences the actions based on the plan from the LLM."]}),"\n",(0,o.jsx)(n.h3,{id:"task_executorpy",children:(0,o.jsx)(n.code,{children:"task_executor.py"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom action_msgs.srv import CancelGoal\nfrom nav2_msgs.action import NavigateToPose\nfrom vision_msgs.msg import Detection3DArray\nfrom manipulation_msgs.action import GraspObject\n\nclass TaskExecutor(Node):\n    def __init__(self):\n        super().__init__('task_executor')\n        self.plan_subscriber = self.create_subscription(\n            String,\n            '/llm/plan',\n            self.plan_callback,\n            10)\n        self.nav_client = rclpy.action.ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        self.manipulation_client = rclpy.action.ActionClient(self, GraspObject, 'grasp_object')\n        self.detection_subscriber = self.create_subscription(\n            Detection3DArray,\n            '/perception/detections',\n            self.detection_callback,\n            10)\n        self.current_plan = []\n        self.get_logger().info('Task Executor node has been started.')\n\n    def plan_callback(self, msg):\n        self.get_logger().info(f'Received plan: {msg.data}')\n        # Simple plan parsing, a real implementation would be more robust\n        self.current_plan = msg.data.split(';')\n        self.execute_next_step()\n\n    def execute_next_step(self):\n        if not self.current_plan:\n            self.get_logger().info('Plan finished.')\n            return\n\n        step = self.current_plan.pop(0).strip()\n        self.get_logger().info(f'Executing step: {step}')\n\n        if step.startswith('navigate'):\n            # Parse target from plan, e.g., \"navigate to table\"\n            target_pose = self.get_pose_from_target(step.split(' to ')[1])\n            self.send_nav_goal(target_pose)\n        elif step.startswith('find'):\n            # e.g., \"find the red cup\"\n            self.target_object = step.split(' the ')[1]\n            # Assumes perception is running and will publish detections\n        elif step.startswith('pick_up'):\n            # Assumes object was found\n            self.send_manipulation_goal(self.last_detected_object)\n\n    def send_nav_goal(self, pose):\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.pose = pose\n        self.nav_client.wait_for_server()\n        self._send_goal_future = self.nav_client.send_goal_async(goal_msg)\n        self._send_goal_future.add_done_callback(self.nav_goal_response_callback)\n\n    def nav_goal_response_callback(self, future):\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info('Nav goal rejected')\n            return\n        self._get_result_future = goal_handle.get_result_async()\n        self._get_result_future.add_done_callback(self.nav_get_result_callback)\n\n    def nav_get_result_callback(self, future):\n        result = future.result().result\n        self.get_logger().info(f'Nav result: {result.success}')\n        self.execute_next_step()\n\n    # ... similar methods for manipulation and detection ...\n\ndef main(args=None):\n    rclpy.init(args=args)\n    executor = TaskExecutor()\n    rclpy.spin(executor)\n    executor.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"integrated-code-blocks",children:"Integrated Code Blocks"}),"\n",(0,o.jsx)(n.p,{children:"Here are some examples of the other nodes that make up the full system."}),"\n",(0,o.jsxs)(n.h4,{id:"cognitive-planner-node-cognitive_planner_nodepy",children:["Cognitive Planner Node (",(0,o.jsx)(n.code,{children:"cognitive_planner_node.py"}),")"]}),"\n",(0,o.jsx)(n.p,{children:"This node interfaces with a large language model to turn a user command into a sequence of robot actions."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport openai  # Or any other LLM client\n\nclass CognitivePlanner(Node):\n    def __init__(self):\n        super().__init__('cognitive_planner')\n        self.command_subscriber = self.create_subscription(\n            String,\n            '/user/command',\n            self.command_callback,\n            10)\n        self.plan_publisher = self.create_publisher(String, '/llm/plan', 10)\n        # Configure your LLM API key\n        # openai.api_key = 'YOUR_API_KEY'\n\n    def command_callback(self, msg):\n        prompt = f\"Convert the following command into a semi-colon separated plan for a robot. The available actions are navigate, find, and pick_up. Command: '{msg.data}'\"\n        \n        try:\n            # This is a mocked response. In a real scenario, you would call the LLM API.\n            # response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=100)\n            # plan = response.choices[0].text.strip()\n            \n            mock_plan = \"navigate to table; find red can; pick_up red can; navigate to user\"\n            self.get_logger().info(f\"Generated plan: {mock_plan}\")\n            \n            plan_msg = String()\n            plan_msg.data = mock_plan\n            self.plan_publisher.publish(plan_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\"Error generating plan: {e}\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    planner = CognitivePlanner()\n    rclpy.spin(planner)\n    planner.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsxs)(n.h4,{id:"manipulation-action-server-manipulation_serverpy",children:["Manipulation Action Server (",(0,o.jsx)(n.code,{children:"manipulation_server.py"}),")"]}),"\n",(0,o.jsx)(n.p,{children:"This node provides an action server for the robot's manipulator, allowing it to pick up objects."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.action import ActionServer\nfrom rclpy.node import Node\nfrom manipulation_msgs.action import GraspObject\nimport time\n\nclass ManipulationServer(Node):\n    def __init__(self):\n        super().__init__('manipulation_server')\n        self._action_server = ActionServer(\n            self,\n            GraspObject,\n            'grasp_object',\n            self.execute_callback)\n\n    def execute_callback(self, goal_handle):\n        self.get_logger().info('Executing grasp...')\n        \n        # In a real robot, this would involve complex motion planning and gripper control.\n        # Here, we simulate the action.\n        time.sleep(5) # Simulate time to grasp\n        \n        goal_handle.succeed()\n        \n        result = GraspObject.Result()\n        result.success = True\n        return result\n\ndef main(args=None):\n    rclpy.init(args=args)\n    manipulation_server = ManipulationServer()\n    rclpy.spin(manipulation_server)\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"running-the-full-simulation",children:"Running the Full Simulation"}),"\n",(0,o.jsx)(n.p,{children:"To run the complete capstone project, you need to launch all the components in the correct order."}),"\n",(0,o.jsxs)(n.h3,{id:"launch-file-capstonelaunchpy",children:["Launch File: ",(0,o.jsx)(n.code,{children:"capstone.launch.py"})]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='isaac_ros_sim',\n            executable='simulation_engine',\n            name='isaac_sim'\n        ),\n        Node(\n            package='slam_toolbox',\n            executable='async_slam_toolbox_node',\n            name='slam_toolbox',\n            parameters=[{'use_sim_time': True}]\n        ),\n        Node(\n            package='nav2_bringup',\n            executable='bringup_launch',\n            name='nav2_bringup'\n        ),\n        Node(\n            package='whisper_ros',\n            executable='whisper_node',\n            name='whisper'\n        ),\n        Node(\n            package='llm_ros',\n            executable='cognitive_planner_node',\n            name='cognitive_planner'\n        ),\n        Node(\n            package='object_detection_ros',\n            executable='detection_node',\n            name='object_detector'\n        ),\n        Node(\n            package='manipulation_ros',\n            executable='manipulation_server',\n            name='manipulation_server'\n        ),\n        Node(\n            package='capstone_pkg',\n            executable='task_executor.py',\n            name='task_executor'\n        ),\n    ])\n"})}),"\n",(0,o.jsx)(n.h3,{id:"steps-to-run",children:"Steps to Run"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Source your ROS 2 workspace:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"source install/setup.bash\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Launch the simulation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"ros2 launch capstone_pkg capstone.launch.py\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Provide a voice command:"}),'\n"Okay robot, please find the red soda can on the table and bring it to me."']}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"example-task-walkthrough",children:"Example Task Walkthrough"}),"\n",(0,o.jsx)(n.p,{children:'Let\'s trace the execution for the command: "bring me the red can".'}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Whisper Node"})," transcribes the audio to text: ",(0,o.jsx)(n.code,{children:'"bring me the red can"'}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"LLM Planner Node"})," receives the text and generates a plan: ",(0,o.jsx)(n.code,{children:'"navigate to table; find red can; pick_up red can; navigate to user"'}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Task Executor"})," receives the plan and starts executing:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"navigate to table"})}),": Sends a goal to Nav2. The robot moves to the table."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"find red can"})}),": The executor waits for the object detection node to publish the pose of the red can."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"pick_up red can"})}),": Once the can is detected, the executor sends a goal to the manipulation action server to grasp the can."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"navigate to user"})}),": After grasping, the robot navigates back to the user's location."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.li,{children:"The robot arrives at the user and waits for the next command."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"capstone-task-flow",children:"Capstone Task Flow"}),"\n",(0,o.jsx)(n.p,{children:"This diagram illustrates the flow of a typical task, from receiving a command to completing the action."}),"\n",(0,o.jsx)(n.mermaid,{value:'sequenceDiagram\n    participant User\n    participant Whisper\n    participant LLMPlanner\n    participant TaskExecutor\n    participant Nav2\n    participant ObjectDetector\n    participant Manipulator\n\n    User->>Whisper: "Bring me the red can"\n    Whisper->>LLMPlanner: "bring me the red can"\n    LLMPlanner->>TaskExecutor: "navigate to table; find red can; pick_up red can; navigate to user"\n    TaskExecutor->>Nav2: NavigateToPose(table)\n    Nav2--\x3e>TaskExecutor: Success\n    TaskExecutor->>ObjectDetector: Activate\n    ObjectDetector--\x3e>TaskExecutor: Pose of red can\n    TaskExecutor->>Manipulator: GraspObject(red can)\n    Manipulator--\x3e>TaskExecutor: Success\n    TaskExecutor->>Nav2: NavigateToPose(user)\n    Nav2--\x3e>TaskExecutor: Success'}),"\n",(0,o.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,o.jsx)(n.p,{children:"This capstone project has successfully integrated a wide range of technologies to create an autonomous humanoid robot capable of understanding and executing natural language commands in a simulated environment. You have learned how to combine ROS 2 with advanced AI for perception, planning, and action, laying a strong foundation for building sophisticated real-world robotic systems. The principles and techniques covered here are at the forefront of modern robotics and will be invaluable in your future work."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>r});var t=a(6540);const o={},i=t.createContext(o);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);