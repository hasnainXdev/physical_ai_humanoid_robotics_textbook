"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[4701],{2352:(e,n,t)=>{t.d(n,{A:()=>_});var i=t(6540),o=t(7489),r=t(2181),a=t(6342),s=t(5293);let l=null;async function c(){return l||(l=async function(){return(await t.e(2279).then(t.bind(t,2279))).default}()),l}const d="docusaurus-mermaid-container";function h(){const{colorMode:e}=(0,s.G)(),n=(0,a.p)().mermaid,t=n.theme[e],{options:o}=n;return(0,i.useMemo)(()=>({startOnLoad:!1,...o,theme:t}),[t,o])}function g({text:e,config:n}){const[t,o]=(0,i.useState)(null),r=(0,i.useState)(`mermaid-svg-${Math.round(1e7*Math.random())}`)[0],a=h(),s=n??a;return(0,i.useEffect)(()=>{(async function({id:e,text:n,config:t}){const i=await c();i.initialize(t);try{return await i.render(e,n)}catch(o){throw document.querySelector(`#d${e}`)?.remove(),o}})({id:r,text:e,config:s}).then(o).catch(e=>{o(()=>{throw e})})},[r,e,s]),t}const u={container:"container_lyt7"};var p=t(4848);function m({renderResult:e}){const n=(0,i.useRef)(null);return(0,i.useEffect)(()=>{const t=n.current;e.bindFunctions?.(t)},[e]),(0,p.jsx)("div",{ref:n,className:`${d} ${u.container}`,dangerouslySetInnerHTML:{__html:e.svg}})}function f({value:e}){const n=g({text:e});return null===n?null:(0,p.jsx)(m,{renderResult:n})}function _(e){return(0,p.jsx)(o.A,{fallback:e=>(0,p.jsx)(r.MN,{...e}),children:(0,p.jsx)(f,{...e})})}},3195:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>g,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module1-ros2/06-ai-integration","title":"Integrating AI Agents with ROS 2 Controllers","description":"1. Bridging the Gap Between AI and Robotics","source":"@site/docs/module1-ros2/06-ai-integration.mdx","sourceDirName":"module1-ros2","slug":"/module1-ros2/06-ai-integration","permalink":"/physical_ai_humanoid_robotics_textbook/docs/module1-ros2/06-ai-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module1-ros2/06-ai-integration.mdx","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"06-ai-integration","title":"Integrating AI Agents with ROS 2 Controllers","sidebar_label":"AI Integration"},"sidebar":"tutorialSidebar","previous":{"title":"URDF Modeling","permalink":"/physical_ai_humanoid_robotics_textbook/docs/module1-ros2/05-urdf-modeling"},"next":{"title":"Introduction","permalink":"/physical_ai_humanoid_robotics_textbook/docs/module2-digital-twins/01-introduction"}}');var o=t(4848),r=t(8453),a=(t(3457),t(2352));const s={id:"06-ai-integration",title:"Integrating AI Agents with ROS 2 Controllers",sidebar_label:"AI Integration"},l=void 0,c={},d=[{value:"1. Bridging the Gap Between AI and Robotics",id:"1-bridging-the-gap-between-ai-and-robotics",level:2},{value:"The AI-Robot Control Loop",id:"the-ai-robot-control-loop",level:3},{value:"2. Communication Between the AI Agent and ROS 2",id:"2-communication-between-the-ai-agent-and-ros-2",level:2},{value:"Why Use an Action Server?",id:"why-use-an-action-server",level:3},{value:"Example: A Simple &quot;Greeter&quot; AI Agent",id:"example-a-simple-greeter-ai-agent",level:3},{value:"Step 1: The Action Definition",id:"step-1-the-action-definition",level:4},{value:"Step 2: The ROS 2 Action Server",id:"step-2-the-ros-2-action-server",level:4},{value:"Step 3: The AI Agent (Client)",id:"step-3-the-ai-agent-client",level:4},{value:"3. Integrating with LLMs and Cognitive Planners",id:"3-integrating-with-llms-and-cognitive-planners",level:2},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const n={code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"1-bridging-the-gap-between-ai-and-robotics",children:"1. Bridging the Gap Between AI and Robotics"}),"\n",(0,o.jsx)(n.p,{children:"The integration of Artificial Intelligence (AI) with robotics is a rapidly growing field. AI agents, particularly those based on Large Language Models (LLMs) and other machine learning techniques, can provide high-level decision-making capabilities to robots. ROS 2, with its robust communication and control infrastructure, is the perfect platform for executing the actions decided by these AI agents."}),"\n",(0,o.jsx)(n.p,{children:"This chapter explores how to connect an AI agent to a ROS 2 system, allowing the AI to control the robot's behavior."}),"\n",(0,o.jsx)(n.h3,{id:"the-ai-robot-control-loop",children:"The AI-Robot Control Loop"}),"\n",(0,o.jsx)(n.p,{children:"The basic control loop for an AI-driven robot can be broken down into the following steps:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception"}),": The robot gathers information about its environment through sensors (cameras, LiDAR, etc.)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"AI Agent"}),": The AI agent processes the perception data, along with a high-level goal, to decide on the next action."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS 2 Controller"}),": The AI agent sends a command to a ROS 2 controller."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Execution"}),": The ROS 2 controller translates the command into low-level actions for the robot's actuators."]}),"\n"]}),"\n",(0,o.jsx)(a.A,{chart:"\ngraph TD\n  A[Sensors] --\x3e B(Perception Data);\n  B --\x3e C{AI Agent};\n  C --\x3e D[Action Command];\n  D --\x3e E(ROS 2 Controller);\n  E --\x3e F[Actuator Commands];\n  F --\x3e G(Robot);\n"}),"\n",(0,o.jsx)(n.h2,{id:"2-communication-between-the-ai-agent-and-ros-2",children:"2. Communication Between the AI Agent and ROS 2"}),"\n",(0,o.jsx)(n.p,{children:"The AI agent and the ROS 2 system can be seen as two separate components that need to communicate effectively. There are several ways to achieve this, but a common approach is to use a ROS 2 action server."}),"\n",(0,o.jsx)(n.h3,{id:"why-use-an-action-server",children:"Why Use an Action Server?"}),"\n",(0,o.jsx)(n.p,{children:'An action server is ideal for this scenario because it\'s designed for long-running, goal-oriented tasks. The AI agent can send a goal to the action server (e.g., "pick up the red block"), and the action server will manage the execution of this task, providing feedback and a final result.'}),"\n",(0,o.jsx)(n.h3,{id:"example-a-simple-greeter-ai-agent",children:'Example: A Simple "Greeter" AI Agent'}),"\n",(0,o.jsx)(n.p,{children:"Let's create a simple example where an AI agent tells a robot to greet a person."}),"\n",(0,o.jsx)(n.h4,{id:"step-1-the-action-definition",children:"Step 1: The Action Definition"}),"\n",(0,o.jsxs)(n.p,{children:["First, we need to define the action. Create a file named ",(0,o.jsx)(n.code,{children:"Greet.action"})," in an ",(0,o.jsx)(n.code,{children:"action"})," directory within your package:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"# Greet.action\nstring name\n---\nstring greeting\n---\nstring status\n"})}),"\n",(0,o.jsx)(n.p,{children:"This defines a goal (the name of the person to greet), a result (the greeting message), and feedback (the current status of the greeting process)."}),"\n",(0,o.jsx)(n.h4,{id:"step-2-the-ros-2-action-server",children:"Step 2: The ROS 2 Action Server"}),"\n",(0,o.jsx)(n.p,{children:"Now, let's create the ROS 2 action server that will receive the goal from the AI agent and execute the greeting."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.action import ActionServer\nfrom rclpy.node import Node\nfrom my_robot_interfaces.action import Greet\n\nclass GreeterActionServer(Node):\n\n    def __init__(self):\n        super().__init__('greeter_action_server')\n        self._action_server = ActionServer(\n            self,\n            Greet,\n            'greet',\n            self.execute_callback)\n\n    def execute_callback(self, goal_handle):\n        self.get_logger().info('Executing goal...')\n\n        feedback_msg = Greet.Feedback()\n        feedback_msg.status = 'Generating greeting...'\n        goal_handle.publish_feedback(feedback_msg)\n\n        # In a real application, you would have more complex logic here\n        greeting = f\"Hello, {goal_handle.request.name}!\"\n\n        goal_handle.succeed()\n\n        result = Greet.Result()\n        result.greeting = greeting\n        return result\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = GreeterActionServer()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h4,{id:"step-3-the-ai-agent-client",children:"Step 3: The AI Agent (Client)"}),"\n",(0,o.jsx)(n.p,{children:"The AI agent will act as the action client. This could be a separate Python script that uses a library like OpenAI's GPT to generate the name to greet. For simplicity, we'll simulate the AI's decision with a simple script."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.action import ActionClient\nfrom rclpy.node import Node\nfrom my_robot_interfaces.action import Greet\n\nclass GreeterAI(Node):\n\n    def __init__(self):\n        super().__init__('greeter_ai')\n        self._action_client = ActionClient(self, Greet, 'greet')\n\n    def send_goal(self, name):\n        goal_msg = Greet.Goal()\n        goal_msg.name = name\n\n        self._action_client.wait_for_server()\n\n        return self._action_client.send_goal_async(goal_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    ai_agent = GreeterAI()\n\n    # --- AI Logic Would Go Here ---\n    # For example, an LLM could decide to greet \"World\"\n    future = ai_agent.send_goal(\"World\")\n    # -----------------------------\n\n    rclpy.spin_until_future_complete(ai_agent, future)\n    ai_agent.get_logger().info('Goal sent!')\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"3-integrating-with-llms-and-cognitive-planners",children:"3. Integrating with LLMs and Cognitive Planners"}),"\n",(0,o.jsx)(n.p,{children:'The "AI Logic" section in the client script is where the real power of AI comes in. You can integrate various AI techniques here:'}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Large Language Models (LLMs)"}),": Use LLMs to understand natural language commands from a user and translate them into goals for the ROS 2 system."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cognitive Planners"}),": For more complex tasks, a cognitive planner can break down a high-level goal into a sequence of smaller, executable actions. Each of these actions can then be sent to a ROS 2 action server."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Computer Vision"}),": Use computer vision models to detect objects and people in the environment, and use this information to inform the AI's decisions."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,o.jsx)(n.p,{children:"Integrating AI agents with ROS 2 controllers opens up a world of possibilities for creating intelligent and autonomous robots. By using ROS 2 actions, you can create a clean and robust interface between the high-level decision-making of your AI and the low-level execution of your robot's hardware. This modular approach allows you to develop and test your AI and robotics systems independently, leading to a more efficient and scalable development process."})]})}function g(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}}}]);