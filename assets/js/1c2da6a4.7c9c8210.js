"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[5271],{410:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"capstone-humanoid-robot/voice-llm-integration","title":"Integrating Voice Commands & LLMs","description":"The Conversational Interface for Humanoid Robots","source":"@site/docs/capstone-humanoid-robot/02-voice-llm-integration.mdx","sourceDirName":"capstone-humanoid-robot","slug":"/capstone-humanoid-robot/voice-llm-integration","permalink":"/physical_ai_humanoid_robotics_textbook/docs/capstone-humanoid-robot/voice-llm-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/capstone-humanoid-robot/02-voice-llm-integration.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Capstone Project Overview: Autonomous Humanoid Robot","permalink":"/physical_ai_humanoid_robotics_textbook/docs/capstone-humanoid-robot/overview"},"next":{"title":"SLAM & Nav2 for Autonomous Navigation","permalink":"/physical_ai_humanoid_robotics_textbook/docs/capstone-humanoid-robot/slam-nav2"}}');var i=o(4848),a=o(8453);const r={},s="Integrating Voice Commands & LLMs",c={},l=[{value:"The Conversational Interface for Humanoid Robots",id:"the-conversational-interface-for-humanoid-robots",level:2},{value:"Voice Command Pipeline: From Speech to Intent",id:"voice-command-pipeline-from-speech-to-intent",level:2},{value:"Architectural Flow (ROS 2 Integration):",id:"architectural-flow-ros-2-integration",level:3},{value:"Prompt Engineering for LLM-Driven Robotics",id:"prompt-engineering-for-llm-driven-robotics",level:2},{value:"Key Prompt Engineering Principles:",id:"key-prompt-engineering-principles",level:3},{value:"Example Prompt Structure for Task Decomposition:",id:"example-prompt-structure-for-task-decomposition",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"integrating-voice-commands--llms",children:"Integrating Voice Commands & LLMs"})}),"\n",(0,i.jsx)(n.h2,{id:"the-conversational-interface-for-humanoid-robots",children:"The Conversational Interface for Humanoid Robots"}),"\n",(0,i.jsx)(n.p,{children:"In the Capstone Project, one of the most crucial aspects of human-robot interaction is the ability for the humanoid to understand and respond to natural language voice commands. This chapter details the integration of speech-to-text capabilities (using models like Whisper) with Large Language Models (LLMs) to create a robust and intuitive conversational interface. The goal is to allow users to issue high-level, abstract instructions to the humanoid, which the robot can then interpret, plan for, and execute within its simulated environment."}),"\n",(0,i.jsx)(n.h2,{id:"voice-command-pipeline-from-speech-to-intent",children:"Voice Command Pipeline: From Speech to Intent"}),"\n",(0,i.jsx)(n.p,{children:"The process of converting spoken language into actionable robotic commands involves a multi-stage pipeline, leveraging the components discussed in Module 4 (VLA Robotics):"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Audio Capture:"})," The simulated humanoid robot's virtual microphones (or actual microphones in a real setup) capture human speech. This audio stream is typically processed to filter noise and segmented into meaningful utterances."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Speech-to-Text (STT) with Whisper:"})," The captured audio is fed into a powerful STT model, such as OpenAI's Whisper. Whisper transcribes the audio into a written text format, handling various accents, languages, and background noise effectively."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Natural Language Understanding (NLU) with LLMs:"})," The transcribed text is then passed to a Large Language Model. The LLM's role is critical here:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Intent Recognition:"})," Identifying the overarching goal or purpose of the user's command (e.g., navigation, manipulation, information retrieval)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Entity Extraction:"}),' Extracting key pieces of information (entities) from the text, such as object names ("red mug"), locations ("kitchen counter"), or specific attributes ("gently").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context Management:"})," Maintaining a dialogue history to understand follow-up questions or commands that refer to previous interactions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Disambiguation:"})," If a command is ambiguous, the LLM can generate a clarifying question back to the user."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Action Plan Generation:"})," Once the intent and entities are clear, the LLM formulates a high-level action plan. This plan breaks down complex instructions into a sequence of simpler, more discrete robotic actions or sub-goals."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"architectural-flow-ros-2-integration",children:"Architectural Flow (ROS 2 Integration):"}),"\n",(0,i.jsx)(n.mermaid,{value:"graph TD\n    A[Human Voice] --\x3e B(Microphone/Simulated Audio)\n    B --\x3e C{ROS 2 Audio Node}\n    C --\x3e D[Whisper STT Node]\n    D --\x3e E[Transcribed Text (ROS 2 String Topic)]\n    E --\x3e F{LLM Interface Node (Cognitive Planner)}\n    \n    subgraph Robot Perception & State\n        G[Robot Sensors] --\x3e H(Perception Nodes)\n        H --\x3e I[World Model / Object States (ROS 2 Topics/Services)]\n        I --\x3e F\n    end\n    \n    F --\x3e J[High-Level Action Plan (ROS 2 Custom Message)]\n    J --\x3e K[Action Executor Nodes (Nav2, MoveIt!/Manipulation)]\n    K --\x3e L[Robot Control Interfaces]\n    L --\x3e M[Humanoid Robot Actuators]\n\n    F -- Clarification Query --\x3e B\n    K -- Execution Feedback --\x3e F"}),"\n",(0,i.jsx)(n.h2,{id:"prompt-engineering-for-llm-driven-robotics",children:"Prompt Engineering for LLM-Driven Robotics"}),"\n",(0,i.jsx)(n.p,{children:"The effectiveness of the LLM in understanding commands and generating plans heavily relies on prompt engineering. Crafting clear, concise, and well-structured prompts is essential to guide the LLM's reasoning process."}),"\n",(0,i.jsx)(n.h3,{id:"key-prompt-engineering-principles",children:"Key Prompt Engineering Principles:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Role Assignment:"}),' Clearly define the LLM\'s role (e.g., "You are a robotic task planner for a humanoid robot.").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Capability Definition:"})," Provide the LLM with a list of the robot's available actions and their parameters (e.g., ",(0,i.jsx)(n.code,{children:"navigate(location)"}),", ",(0,i.jsx)(n.code,{children:"grasp(object_id)"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Contextual Information:"})," Feed the LLM with the current state of the robot and the environment (e.g., detected objects and their poses, current location)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output Format:"})," Specify the desired output format for the action plan (e.g., a numbered list of steps, a JSON object representing a sequence of function calls)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Chain of Thought (CoT):"})," Encourage the LLM to explain its reasoning process, which can help in debugging and improving plan quality."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"example-prompt-structure-for-task-decomposition",children:"Example Prompt Structure for Task Decomposition:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:'"""\nYou are an intelligent task planner for a humanoid robot named \'Apollo\'.\nApollo can perform the following high-level actions:\n- navigate(location_name): Move to a predefined location.\n- grasp(object_name): Pick up an object. Requires object to be visible and reachable.\n- release(object_name): Place down a held object.\n- find(object_name): Search for a specific object in the current area.\n\nCurrent environment context: Apollo is in the living room. Visible objects: red_ball, blue_cube, remote_control. Locations: kitchen, bedroom, living_room.\n\nBased on the following user command, provide a step-by-step plan using only the actions listed above. If an object needs to be found, include a \'find\' step. Be precise with object and location names.\n\nUser Command: "Apollo, go to the kitchen, find the coffee cup, and bring it to me."\n\nPlan:\n1. navigate(kitchen)\n2. find(coffee_cup)\n3. grasp(coffee_cup)\n4. navigate(human_location) # Assuming \'human_location\' is tracked\n5. release(coffee_cup)\n"""\n'})}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"Integrating voice commands with LLMs provides a powerful and natural interface for controlling humanoid robots in the Capstone Project. By meticulously designing the pipeline from speech-to-text to intelligent plan generation, and employing effective prompt engineering, we can enable the humanoid robot to understand complex human instructions and execute tasks autonomously, marking a significant step towards truly intelligent human-robot collaboration."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>s});var t=o(6540);const i={},a=t.createContext(i);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);